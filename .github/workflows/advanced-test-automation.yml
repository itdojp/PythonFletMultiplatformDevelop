name: Advanced Test Automation

on:
  push:
    branches:
      - main
      - develop
    paths:
      - 'src/**'
      - 'tests/**'
      - '.github/workflows/**'
  pull_request:
    branches:
      - main
      - develop
  schedule:
    - cron: '0 0 * * 1'  # 毎週月曜日の午前0時

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      test_environment: ${{ steps.setup-env.outputs.test_environment }}
      test_data_path: ${{ steps.setup-env.outputs.test_data_path }}
      test_config_path: ${{ steps.setup-env.outputs.test_config_path }}

    steps:
    - uses: actions/checkout@v3

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt

    - name: Setup test environment
      id: setup-env
      run: |
        python -c """
        import json
        from backend.tests.environment.test_environment_extended import TestEnvironment
        from backend.tests.config.test_config import TestSettings

        env = TestEnvironment(TestEnvironmentType.CI)
        setup_result = env.setup_environment()

        print(f'::set-output name=test_environment::{json.dumps(setup_result)}')
        print(f'::set-output name=test_data_path::{TestSettings.get_test_data_path()}')
        print(f'::set-output name=test_config_path::{TestSettings.get_test_config_path()}')
        """

  test:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.8', '3.9', '3.10']
        test-type: ['unit', 'integration', 'performance']

    steps:
    - uses: actions/checkout@v3

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt

    - name: Run tests
      env:
        TEST_ENVIRONMENT: ${{ needs.setup.outputs.test_environment }}
        TEST_DATA_PATH: ${{ needs.setup.outputs.test_data_path }}
        TEST_CONFIG_PATH: ${{ needs.setup.outputs.test_config_path }}
      run: |
        python -c """
        import json
        from backend.tests.environment.test_environment_extended import TestEnvironment

        env = TestEnvironment(TestEnvironmentType.CI)
        test_result = env.run_tests(
            test_type=${{ matrix.test-type }},
            parallel=True
        )

        with open('test-results.json', 'w') as f:
            json.dump(test_result, f)
        """

    - name: Upload test results
      uses: actions/upload-artifact@v3
      with:
        name: test-results-${{ matrix.python-version }}-${{ matrix.test-type }}
        path: test-results.json

  analyze:
    needs: test
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Download test results
      uses: actions/download-artifact@v3
      with:
        name: test-results
        path: test-results

    - name: Analyze test results
      run: |
        python -c """
        import json
        import os
        from backend.tests.environment.test_environment_extended import TestEnvironment

        env = TestEnvironment(TestEnvironmentType.CI)
        results = env.collect_results()

        with open('analysis-results.json', 'w') as f:
            json.dump(results, f)
        """

    - name: Generate reports
      run: |
        python -c """
        import json
        from datetime import datetime

        with open('analysis-results.json') as f:
            results = json.load(f)

        with open('test-report.md', 'w') as f:
            f.write('# Test Report\n\n')
            f.write(f'Generated at: {datetime.now().isoformat()}\n\n')
            f.write('## Environment\n\n')
            f.write(f'- Environment Type: {results["environment"]}\n')
            f.write('## Test Results\n\n')
            f.write(f'- Status: {results["test_results"]["status"]}\n')
            f.write(f'- Total Tests: {results["test_results"]["total_tests"]}\n')
            f.write(f'- Passed: {results["test_results"]["passed_tests"]}\n')
            f.write(f'- Failed: {results["test_results"]["failed_tests"]}\n')
            f.write('## Coverage\n\n')
            f.write(f'- Total: {results["coverage"]["total"]}%\n')
            f.write(f'- Missing: {results["coverage"]["missing"]}\n')
            f.write('## Performance\n\n')
            f.write(f'- Execution Time: {results["performance"]["execution_time"]}s\n')
            f.write(f'- Response Time: {results["performance"]["response_time"]}ms\n')
        """

    - name: Upload reports
      uses: actions/upload-artifact@v3
      with:
        name: test-reports
        path: test-report.md

  notify:
    needs: analyze
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Download reports
      uses: actions/download-artifact@v3
      with:
        name: test-reports
        path: test-reports

    - name: Notify Slack
      uses: rtCamp/action-slack-notify@v2
      with:
        webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
        status: ${{ needs.analyze.result }}
        title: 'Test Results'
        text: |
          Test Results Summary:
          - Environment: ${{ needs.analyze.outputs.environment }}
          - Status: ${{ needs.analyze.outputs.status }}
          - Total Tests: ${{ needs.analyze.outputs.total_tests }}
          - Passed: ${{ needs.analyze.outputs.passed_tests }}
          - Failed: ${{ needs.analyze.outputs.failed_tests }}
          - Coverage: ${{ needs.analyze.outputs.coverage }}%
          - Execution Time: ${{ needs.analyze.outputs.execution_time }}s
